{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da4eefa-c561-4a6b-8165-26dbd88c3a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "df_locations = spark.table(\"air_quality.bronze.air_quality_locations_bronze\")\n",
    "df_sensors = spark.table(\"air_quality.bronze.air_quality_sensors_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07554b3-e5c7-4d12-a32a-873f5f804078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode sensors array in locations to join with sensor data\n",
    "from pyspark.sql.functions import explode, col\n",
    "# Explode sensors array in locations to join with sensor data\n",
    "df_sensors_flat = df_locations.select(\n",
    "\n",
    "    \"id\", \"name\", \"locality\", \"timezone\", \"country\", \"coordinates\",\n",
    "    explode(\"sensors\").alias(\"sensor\")\n",
    ").select(\n",
    "    col(\"id\").alias(\"location_id\"),\n",
    "    col(\"name\").alias(\"location_name\"),\n",
    "    col(\"locality\"),\n",
    "    col(\"timezone\"),\n",
    "    col(\"country.code\").alias(\"country_code\"),\n",
    "    col(\"coordinates.latitude\").alias(\"latitude\"),\n",
    "    col(\"coordinates.longitude\").alias(\"longitude\"),\n",
    "    col(\"sensor.id\").alias(\"sensor_id\"),\n",
    "    col(\"sensor.name\").alias(\"sensor_name\"),\n",
    "    col(\"sensor.parameter.id\").alias(\"parameter_id\"),\n",
    "    col(\"sensor.parameter.name\").alias(\"parameter_name\"),\n",
    "    col(\"sensor.parameter.units\").alias(\"parameter_units\"),\n",
    "    col(\"sensor.parameter.displayName\").alias(\"parameter_display_name\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6e3f25-3548-4b95-a5fb-639b6243f10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode coverage info if needed\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert coverage dictionary to multiple columns\n",
    "df_sensors_measured = df_sensors.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"parameter\",\n",
    "    \"coverage\",\n",
    "    \"datetimeFirst\",\n",
    "    \"datetimeLast\"\n",
    ")\n",
    "\n",
    "# Example: extract coverage percentages as numbers\n",
    "df_sensors_measured = df_sensors_measured.withColumn(\n",
    "    \"percent_coverage\",\n",
    "    col(\"coverage.percentCoverage\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"observed_count\",\n",
    "    col(\"coverage.observedCount\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"expected_count\",\n",
    "    col(\"coverage.expectedCount\").cast(\"double\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9efe2e8-c020-456c-ba64-d262fa76c4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sensors_measured = df_sensors_measured \\\n",
    "    .withColumn(\"datetime_first\", F.to_timestamp(col(\"datetimeFirst.utc\"))) \\\n",
    "    .withColumn(\"datetime_last\", F.to_timestamp(col(\"datetimeLast.utc\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa9531b-8799-4094-8691-f9040a1ed993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "coverage_schema = StructType([\n",
    "    StructField(\"expectedCount\", DoubleType(), True),\n",
    "    StructField(\"expectedInterval\", StringType(), True),\n",
    "    StructField(\"observedCount\", DoubleType(), True),\n",
    "    StructField(\"observedInterval\", StringType(), True),\n",
    "    StructField(\"percentComplete\", DoubleType(), True),\n",
    "    StructField(\"percentCoverage\", DoubleType(), True),\n",
    "    StructField(\"datetimeFrom\", StructType([\n",
    "        StructField(\"utc\", StringType(), True),\n",
    "        StructField(\"local\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"datetimeTo\", StructType([\n",
    "        StructField(\"utc\", StringType(), True),\n",
    "        StructField(\"local\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8f2b6d-ba5a-4d91-87d9-463ea15114c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df_sensors_parsed = df_sensors.withColumn(\n",
    "    \"coverage_struct\",\n",
    "    from_json(\"coverage\", coverage_schema)\n",
    ")\n",
    "\n",
    "df_sensors_parsed = df_sensors_parsed.withColumn(\n",
    "    \"percent_coverage\", col(\"coverage_struct.percentCoverage\")\n",
    ").withColumn(\n",
    "    \"observed_count\", col(\"coverage_struct.observedCount\")\n",
    ").withColumn(\n",
    "    \"expected_count\", col(\"coverage_struct.expectedCount\")\n",
    ")\n",
    "\n",
    "df_sensors_parsed = df_sensors_parsed \\\n",
    "    .withColumn(\"datetime_first\", F.to_timestamp(col(\"datetimeFirst.utc\"))) \\\n",
    "    .withColumn(\"datetime_last\", F.to_timestamp(col(\"datetimeLast.utc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910f8607-4a70-45d9-b2cb-55aa96e2959b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_sensors_flat.join(\n",
    "    df_sensors_parsed,\n",
    "    df_sensors_flat.sensor_id == df_sensors_parsed.id,\n",
    "    how=\"left\"\n",
    ").drop(df_sensors_parsed.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5342f651-5eb0-422c-ae6a-739df568cfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_clean_and_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
